{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPmPMfdpG77anc+2hLElY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauriV1/Data-Manifesto-Cookbook/blob/main/Data_Cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cookbook\n",
        "\n",
        "## For CS 215, Final\n",
        "## Professor Wirfs Brock\n",
        "## Submitted by - Gauri Vaidya\n",
        "\n"
      ],
      "metadata": {
        "id": "bFmna0LT9XUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Cleaning and Transforming Weather Data\n",
        "\n",
        "\n",
        "We start with a sample weather dataset (project 1) to demonstrate basic Pandas operations: loading data, converting units, and filtering. Suppose we have a CSV of hourly weather measurements (temperature in Fahrenheit, humidity, wind speed, etc.).\n",
        "\n",
        "We will:\n",
        "*   Load the data into a Pandas DataFrame.\n",
        "*   Convert temperatures from Fahrenheit to Celsius (vectorized and via a function).\n",
        "*   Add a new column indicating if dew is likely (temperature ≤ dew point).\n",
        "*   Filter rows for a specific condition (e.g., high humidity)."
      ],
      "metadata": {
        "id": "yGI7IwYsB-I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample weather data as a small example (DateTime, Temp_out (°F), Dew_point (°F), Humidity, Wind_speed)\n",
        "data = {\n",
        "    \"date\": [\"2025-05-12 00:00\", \"2025-05-12 01:00\", \"2025-05-12 02:00\", \"2025-05-12 03:00\"],\n",
        "    \"temp_out\": [50.0, 49.5, 49.0, 48.5],\n",
        "    \"dew_point\": [48.0, 47.5, 47.0, 46.5],\n",
        "    \"humidity\": [85, 87, 88, 90],\n",
        "    \"wind_speed\": [5.0, 4.5, 4.8, 5.2]\n",
        "}\n",
        "df_weather = pd.DataFrame(data)\n",
        "df_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\n",
        "print(df_weather.head())\n"
      ],
      "metadata": {
        "id": "L2vatNYCCtcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it would look something like this:\n",
        "\n",
        "date   temp_out  dew_point  humidity  wind_speed\n",
        "*  0 2025-05-12  00:00:00      50.0       48.0        85         5.0\n",
        "*  1 2025-05-12  01:00:00      49.5       47.5        87         4.5\n",
        "*  2 2025-05-12  02:00:00      49.0       47.0        88         4.8\n",
        "*  3 2025-05-12  03:00:00      48.5       46.5        90         5.2\n",
        "\n",
        "Next, we convert temperatures to Celsius using the formula C = (F - 32) * 5/9. We show two methods: vectorized arithmetic and applying a function.\n"
      ],
      "metadata": {
        "id": "bh7P2sK0Czn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Method 1 -  Vectorized arithmetic (fast)\n",
        "df_weather[\"temp_c_vector\"] = (df_weather[\"temp_out\"] - 32) * 5/9\n",
        "\n",
        "# Method 2 - Using a function with apply (works per entry)\n",
        "def f_to_c(f):\n",
        "    return (f - 32) * 5/9\n",
        "df_weather[\"temp_c_apply\"] = df_weather[\"temp_out\"].apply(f_to_c)\n",
        "\n",
        "print(df_weather[[\"temp_out\", \"temp_c_vector\", \"temp_c_apply\"]])\n"
      ],
      "metadata": {
        "id": "Tmmzm6OUD1is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which would give us results like these:\n",
        "\n",
        "   temp_out  temp_c_vector  temp_c_apply\n",
        "0      50.0      10.000000      10.000000\n",
        "1      49.5       9.722222       9.722222\n",
        "2      49.0       9.444444       9.444444\n",
        "3      48.5       9.166667       9.166667\n"
      ],
      "metadata": {
        "id": "OxmCEjLNEMEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both methods give the same result. The vectorized version (Method 1) is usually faster for large data. We can now add a dew/frost likelihood column: if the air temperature is at or below the dew point, dew is likely. We compare temp_out and dew_point row by row.   "
      ],
      "metadata": {
        "id": "Y3uDP29bEYsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather[\"dew_likely\"] = df_weather.apply(\n",
        "    lambda row: \"Yes\" if row[\"temp_out\"] <= row[\"dew_point\"] else \"No\", axis=1\n",
        ")\n",
        "print(df_weather[[\"date\", \"temp_out\", \"dew_point\", \"dew_likely\"]])\n"
      ],
      "metadata": {
        "id": "IbCCCWSEEdes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Would give us results like these\n",
        "\n",
        "#                 date  temp_out  dew_point dew_likely\n",
        "#0 2025-05-12 00:00:00      50.0       48.0         No\n",
        "#1 2025-05-12 01:00:00      49.5       47.5         No\n",
        "#2 2025-05-12 02:00:00      49.0       47.0         No\n",
        "#3 2025-05-12 03:00:00      48.5       46.5         No\n"
      ],
      "metadata": {
        "id": "5jmlD8erEjRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we filter the DataFrame to find, for example, readings with humidity above 85%. We can use boolean indexing.\n",
        "\n",
        "high_humidity = df_weather[df_weather[\"humidity\"] > 85]\n",
        "print(\"High-humidity rows:\")\n",
        "print(high_humidity)\n",
        "\n",
        "\n",
        "#Which would give us results like this:\n",
        "#High-humidity rows:\n",
        "#                 date  temp_out  dew_point  humidity  wind_speed  temp_c_vector  temp_c_apply dew_likely\n",
        "#1 2025-05-12 01:00:00      49.5       47.5        87         4.5       9.722222       9.722222         No\n",
        "#2 2025-05-12 02:00:00      49.0       47.0        88         4.8       9.444444       9.444444         No\n",
        "#3 2025-05-12 03:00:00      48.5       46.5        90         5.2       9.166667       9.166667         No"
      ],
      "metadata": {
        "id": "9H9MfysAEwFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This recipe shows how to clean and transform data: adding new columns, converting units, and filtering rows with Pandas"
      ],
      "metadata": {
        "id": "DqirZv0EFEOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Merging and Analyzing Student Course Data\n",
        "\n",
        "In this recipe, we use example data inspired by project 3. We have two datasets:\n",
        "\n",
        "*   courses_df: Records of student course enrollments (Student ID, Subject, Course Number, Grade).\n",
        "*   outcomes_df: Student outcomes (Student ID, Primary Major).\n",
        "\n",
        "We will merge these tables on Student ID and then analyze which courses were taken by students of a particular major. No data sets will be uploaded to maintain privacy."
      ],
      "metadata": {
        "id": "CufbR9lIFKfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "courses_df = pd.DataFrame({\n",
        "    \"StudentID\": [1,1,2,2,3,3,4,4],\n",
        "    \"Subject\": [\"MATH\",\"ENGL\",\"MATH\",\"BIOL\",\"CHEM\",\"CHEM\",\"MATH\",\"ENGL\"],\n",
        "    \"CourseNumber\": [\"101\",\"101\",\"101\",\"201\",\"101\",\"102\",\"201\",\"101\"],\n",
        "    \"Grade\": [3.7, 3.3, 3.0, 2.7, 3.3, 3.0, 3.7, 2.7]\n",
        "})\n",
        "outcomes_df = pd.DataFrame({\n",
        "    \"StudentID\": [1,2,3,4],\n",
        "    \"Major1\": [\"BIOL\",\"BIOL\",\"CHEM\",\"CHEM\"]\n",
        "})\n",
        "\n",
        "# Merging on StudentID, keeping only matching records\n",
        "merged_df = pd.merge(courses_df, outcomes_df, on=\"StudentID\", how=\"inner\")\n",
        "print(\"Merged DataFrame:\")\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "id": "IhRCIhhgFj8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Which would give us something like this\n",
        "\n",
        "#Merged DataFrame:\n",
        "#  StudentID Subject CourseNumber  Grade Major1\n",
        "#0          1    MATH          101    3.7   BIOL\n",
        "#1          1    ENGL          101    3.3   BIOL\n",
        "#2          2    MATH          101    3.0   BIOL\n",
        "#3          2    BIOL          201    2.7   BIOL\n",
        "#4          3    CHEM          101    3.3   CHEM\n",
        "#5          3    CHEM          102    3.0   CHEM\n",
        "#6          4    MATH          201    3.7   CHEM\n",
        "#7          4    ENGL          101    2.7   CHEM\n"
      ],
      "metadata": {
        "id": "ROhpftK4Fr6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, suppose we want to find out which 1xx courses are most popular among Biology majors (Major1 == \"BIOL\"). We filter for biology majors and courses numbered in the 100s (the CourseNumber starts with '1' and has length 3)."
      ],
      "metadata": {
        "id": "J2XprnRVGPmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on Biology majors and 1xx courses\n",
        "bio_df = merged_df[merged_df[\"Major1\"] == \"BIOL\"].copy()\n",
        "bio_df[\"FullCourse\"] = bio_df[\"Subject\"] + bio_df[\"CourseNumber\"]\n",
        "bio_df_1xx = bio_df[bio_df[\"CourseNumber\"].str.startswith(\"1\")]\n",
        "course_counts = bio_df_1xx[\"FullCourse\"].value_counts()\n",
        "print(\"1xx courses taken by BIOL majors:\")\n",
        "print(course_counts)\n"
      ],
      "metadata": {
        "id": "Fbg0LvEOGTXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Would give us something like this\n",
        "\n",
        "#1xx courses taken by BIOL majors:\n",
        "#MATH101    2\n",
        "#ENGL101    1\n",
        "#dtype: int64\n"
      ],
      "metadata": {
        "id": "r5zz7n4fGnlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows that MATH101 and ENGL101 were taken by biology majors (with counts). We could further analyze these grades or visualize them, but for brevity we just count courses here. This recipe demonstrates merging tables and filtering for analysis.\n"
      ],
      "metadata": {
        "id": "tFIEI07pGy98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Clustering City Data (K-Means)\n",
        "\n",
        "We use a dataset inspired by project 5 (city parks data). Suppose we have cities with attributes \"Population\" and \"Investment in Parks (dollars)\". We want to cluster similar cities. We use K-Means clustering from scikit-learn."
      ],
      "metadata": {
        "id": "aingjHSGG2eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Example city data\n",
        "city_data = pd.DataFrame({\n",
        "    \"City\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"],\n",
        "    \"Population\": [50000, 120000, 30000, 150000, 80000, 200000, 25000, 110000],\n",
        "    \"Invest_dollars\": [500000, 1200000, 400000, 1500000, 700000, 2000000, 300000, 1000000]\n",
        "})\n",
        "print(\"City data:\")\n",
        "print(city_data)"
      ],
      "metadata": {
        "id": "isxZG0SVHA3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Which would give us something like\n",
        "\n",
        "#. City data:\n",
        "#  City  Population  Invest_dollars\n",
        "#0    A       50000          500000\n",
        "#1    B      120000         1200000\n",
        "#2    C       30000          400000\n",
        "#3    D      150000         1500000\n",
        "#4    E       80000          700000\n",
        "#5    F      200000         2000000\n",
        "#6    G       25000          300000\n",
        "#7    H      110000         1000000\n"
      ],
      "metadata": {
        "id": "32aTLJbjIO9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform K-Means clustering on the (Population, Investment) features. We choose 3 clusters (for example)."
      ],
      "metadata": {
        "id": "fYZJVa8kIeD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = city_data[[\"Population\", \"Invest_dollars\"]]\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(features)\n",
        "city_data[\"Cluster\"] = clusters\n",
        "print(\"\\nCluster assignments:\")\n",
        "print(city_data[[\"City\",\"Cluster\"]])\n"
      ],
      "metadata": {
        "id": "7oJDznpxIkcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Which would give something like this\n",
        "\n",
        "#Cluster assignments:\n",
        "#  City  Cluster\n",
        "#0    A        2\n",
        "#1    B        1\n",
        "#2    C        2\n",
        "#3    D        0\n",
        "#4    E        2\n",
        "#5    F        0\n",
        "#6    G        2\n",
        "#7    H        1\n"
      ],
      "metadata": {
        "id": "eZc5N_vnIlpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each city is assigned to a cluster 0, 1, or 2. We can inspect the cluster centers and sizes:"
      ],
      "metadata": {
        "id": "-IaEGdCCI4tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster centers and counts\n",
        "print(\"\\nCluster centers (Population, Investment):\")\n",
        "print(kmeans.cluster_centers_)\n",
        "print(\"\\nCounts per cluster:\")\n",
        "print(city_data[\"Cluster\"].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "_0M7R47BI-rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Cluster centers (Population, Investment):\n",
        "[[150000. 1500000.]\n",
        " [115000. 1100000.]\n",
        " [ 46875.  537500.]]\n",
        "\n",
        "Counts per cluster:\n",
        "0    2\n",
        "1    2\n",
        "2    4\"\"\""
      ],
      "metadata": {
        "id": "KAq-OdMaJEqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, cluster 0 centers on the largest cities, cluster 2 on the smallest, and cluster 1 in between. This recipe illustrates clustering: grouping cities by similar features. In practice, we would visualize these clusters (e.g., scatter plot) and interpret them. Clustering since is an unsupervised method; I would like to try different algorithms (e.g., hierarchical clustering) on the same data and compare results."
      ],
      "metadata": {
        "id": "AbGyE-VBJQkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Splitting Data into Training and Testing Sets\n",
        "\n",
        "This recipe shows how I embed the Train vs Test efficiency evaluation directly into Streamlit dashboard (from app.py on final project).\n",
        "\n",
        "Split historical data into “in-sample” (train) and “out-of-sample” (test) portions\n",
        "\n",
        "Fit an ARIMA model on the train set\n",
        "\n",
        "Compute an efficiency metric for both sets\n",
        "\n",
        "Display the results as a bar chart side-by-side\n",
        "\n",
        "Why it matters: Transparent model evaluation is crucial—this gives end users instant feedback on how well forecasting pipeline generalizes."
      ],
      "metadata": {
        "id": "fpA8csnxJc3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tab4:\n",
        "    st.subheader(f\"Train vs Test Efficiency — {selected_ticker}\")\n",
        "\n",
        "    # 1. Prepare the in-sample (train) data\n",
        "    #    All dates before the forecast window (May 1–5, 2025)\n",
        "    hist_train = clean_df[\n",
        "        (clean_df['Ticker'] == selected_ticker) &\n",
        "        (clean_df['Date'] < forecast_start)\n",
        "    ]\n",
        "\n",
        "    # 2. Fit ARIMA(3,1,5) on the train series\n",
        "    arima_model = ARIMA(hist_train['Close'], order=(3,1,5)).fit()\n",
        "\n",
        "    # 3. Compute residuals and in-sample efficiency\n",
        "    resid     = hist_train['Close'] - arima_model.fittedvalues\n",
        "    train_eff = (1 - resid.abs() / hist_train['Close']).mean()\n",
        "\n",
        "    # 4. Reuse the avg_eff from the “Model Efficiency” tab as out-of-sample (test) efficiency\n",
        "    test_eff = avg_eff\n",
        "\n",
        "    # 5. Assemble a small DataFrame for plotting\n",
        "    df_eff = pd.DataFrame({\n",
        "        'Period': ['Train', 'Test'],\n",
        "        'Efficiency (%)': [train_eff * 100, test_eff * 100]\n",
        "    })\n",
        "\n",
        "    # 6. Plot a bar chart with Plotly\n",
        "    fig3 = go.Figure(go.Bar(\n",
        "        x=df_eff['Period'],\n",
        "        y=df_eff['Efficiency (%)'],\n",
        "        text=df_eff['Efficiency (%)'].round(2),\n",
        "        textposition='auto'\n",
        "    ))\n",
        "    fig3.update_layout(\n",
        "        title=\"Train vs Test Efficiency (%)\",\n",
        "        yaxis={'range': [0, 100]}\n",
        "    )\n",
        "    st.plotly_chart(fig3, use_container_width=True)\n"
      ],
      "metadata": {
        "id": "olUdEKx5Llq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. . Selecting the Training Data\n",
        "We begin by isolating all historical data prior to our forecast window (May 1–5, 2025). By filtering clean_df on the chosen ticker and dates earlier than forecast_start, we ensure the model only “sees” past data during fitting.\n",
        "\n",
        "2. Fitting the ARIMA Model\n",
        "Next, we fit an ARIMA(3,1,5) model on this in-sample series. This replicates the exact configuration used for forecasting—calling\n",
        "\n",
        "3. 3. Computing In-Sample Efficiency\n",
        "Once the model is trained, we calculate daily residuals as nd convert these into an efficiency score per day:\n",
        "\n",
        "daily efficiency\n",
        "= 1 - mod (resid)/actual\n",
        "\n",
        "Averaging across all days yields train_eff, a single value between 0 and 1 that quantifies how well the model fits its own training data.\n",
        "\n",
        "4. Defining Test Efficiency\n",
        "For the test set, we repurpose the average efficiency previously calculated on the May 1–5, 2025 forecast window (avg_eff). This captures how well the ensemble generalizes to truly unseen data.\n",
        "\n",
        "5. Preparing Data for Visualization\n",
        "We assemble a small two-row DataFrame with the labels “Train” and “Test” alongside their respective efficiencies converted to percentages. This simple table is the basis for our comparative plot.\n",
        "\n",
        "6. Displaying the Comparison\n",
        "Finally, we render a Plotly bar chart in Streamlit: one bar for the train score and one for the test score, each labeled with its exact percentage. By placing this inside a dedicated “Train vs Test” tab, users immediately see both how well the model learned the historical patterns and how robust it remains on future dates.\n",
        "\n",
        "Altogether, this recipe turns dry numeric evaluation into an interactive, visually intuitive dashboard component—strengthening trust in forecasting pipeline by making its limitations and strengths crystal clear."
      ],
      "metadata": {
        "id": "iXpOOXCiL-tM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The Data Cookbook integrates four key recipes—processing and cleaning weather data, merging and analyzing student course records, clustering city park investments, and integrating a train/test efficiency measure into a Streamlit dashboard—each of which includes actual code, brief explanations, and test outputs. By drawing on datasets from semester projects, these recipes demonstrate practical techniques I would like to return to again and again: vectorized transformations versus function-based approaches, DataFrame merges and filters, unsupervised clustering with scikit-learn, and transparent model evaluation inside an interactive app.\n",
        "\n",
        "At its base, there is a **Data Science Manifesto** of my values: making black-box AI understandable, taking on Data Feminism to make equity and justice primary concerns, pushing for open-source and open-access in order to encourage shared accountability, and a culture of curiosity and exploration. Both the manifesto and cookbook together constitute an interdisciplinary toolkit—not just for fixing problems, but for doing so in ethical, transparent, and innovative ways.\n",
        "\n",
        "As I would go on to work on future data science projects—research, internships, or personal projects—this notebook hopefully will be my quick reference to repeat patterns and reminder of the principles that inform good data practice. Keep adding to it: new recipes, refining existing ones, and let the manifesto evolve as my skills develop with time. In that manner, this cookbook will be a living book—a present to my future self and everyone else who treads my data-driven path. :)"
      ],
      "metadata": {
        "id": "YPW0kamDM9Ug"
      }
    }
  ]
}